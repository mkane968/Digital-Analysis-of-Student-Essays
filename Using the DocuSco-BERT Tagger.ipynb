{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the DocuSco-Bert Tagger\n",
    "\n",
    "Analyze a corpus of student essays (sentence or paragraphs) using the David Brown's DocuSco-Bert Tagger. \n",
    "See documentation here: https://huggingface.co/browndw/docusco-bert?text=My+name+is+Clara+and+I+live+in+Berkeley%2C+California. \n",
    "\n",
    "Files needed: \n",
    "* CSV containing student essays and score data (one essay and score per row)\n",
    "\n",
    "Limits: \n",
    "* Accepts documents of max length 512 (info on max length and truncation [here](https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f) and [here.](https://stackoverflow.com/questions/65246703/how-does-max-length-padding-and-truncation-arguments-work-in-huggingface-bertt)\n",
    "* Performs word piece tokenization which splits words and labels each piece with a LAT (redundant). Current workaround is to remove repeated pieces manually. Issue discussed [here.](https://stackoverflow.com/questions/62082938/how-to-stop-bert-from-breaking-apart-specific-words-into-word-piece)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load packages\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "path = os.chdir(\"/Users/megankane/Desktop/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load DocuScope model\n",
    "#https://huggingface.co/browndw/docusco-bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"browndw/docusco-bert\", truncation=True, max_length=512)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"browndw/docusco-bert\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DocuSco-BERT tagger on example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define example sentence\n",
    "example = \"Globalization is the process of interaction and integration among people, companies, and governments worldwide.\"\n",
    "\n",
    "#Run nlp pipeline with DocuSco-BERT tagger on example sentence\n",
    "ds_results = nlp(example)\n",
    "\n",
    "#Print results\n",
    "print(ds_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put results into a dataframe\n",
    "lat_df = pd.DataFrame(ds_results)\n",
    "lat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove any strings that contain ## (redundant)\n",
    "lat_df = lat_df[lat_df[\"word\"].str.contains(\"##\") == False]\n",
    "\n",
    "##Remove b and i from each colum\n",
    "#Data were split into chunks that don't split B + I sequences \n",
    "#and end with sentence-final punctuation marks (i.e., period, quesiton mark or exclamaiton point).\n",
    "lat_df['entity'] = lat_df['entity'].str.replace('B-','')\n",
    "lat_df['entity'] = lat_df['entity'].str.replace('I-','')\n",
    "\n",
    "#Now we have a workable dataframe of LATS\n",
    "lat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download df to csv\n",
    "lat_df.to_csv('lat_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DocuSco-BERT on corpus of student essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload dataframe of student essay paragraphs\n",
    "df = pd.read_csv(r'rhetorical_sentences.csv', index_col=0)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create list for dfs\n",
    "list_of_dfs = []\n",
    "\n",
    "#Create for loop to run DocuSco-BERT on each text in dataframe\n",
    "for doc in df['Text']:\n",
    "    result = nlp(doc)\n",
    "    #Create new dataframe to contain results of each tagged document\n",
    "    lat_df = pd.DataFrame(result)\n",
    "    #Remove any redundant lats from split words\n",
    "    lat_df = lat_df[lat_df[\"word\"].str.contains(\"##\") == False]\n",
    "    #display(df.head())\n",
    "    #Append each dataframe to list of dataframes\n",
    "    list_of_dfs.append(lat_df)\n",
    "    #Print list of length (to track progress)\n",
    "    print(len(list_of_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check on a one of the tagged dataframes\n",
    "list_of_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate list of dfs to one dataframe\n",
    "lat_dfs = pd.concat(list_of_dfs)\n",
    "\n",
    "#Remove b and i from each colum\n",
    "#Data were split into chunks that don't split B + I sequences \n",
    "#and end with sentence-final punctuation marks (i.e., period, quesiton mark or exclamaiton point).\n",
    "lat_dfs['entity'] = lat_df['entity'].str.replace('B-','')\n",
    "lat_dfs['entity'] = lat_df['entity'].str.replace('I-','')\n",
    "lat_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download list of dataframes to csv\n",
    "lat_dfs.to_csv('filename.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create master dataframe with LATs as strings from each document\n",
    "\n",
    "#Append names of LATs in each dataframe to master list\n",
    "results_list = []\n",
    "\n",
    "#Append each LAT column to list and then to list of lists\n",
    "for df in list_of_dfs:\n",
    "  l = df['entity'].to_list()\n",
    "  result = ' '.join(str(item) for item in l)\n",
    "  result = result.replace('nan', '')\n",
    "  results_list.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make list for filenames/scores\n",
    "scores = df['Score_ID_Sentence']\n",
    "\n",
    "#Add list of lists to dataframe\n",
    "results_list\n",
    "\n",
    "result_df = pd.DataFrame(results_list)\n",
    "\n",
    "#Add scores/filenames to dataframe\n",
    "result_df['Scores'] = scores\n",
    "\n",
    "#result_df\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorganize columns \n",
    "final_lat_df = result_df[['Scores', 0]]\n",
    "final_lat_df.columns.values[1] = \"LAT Strings\"\n",
    "\n",
    "#Remove b and i from each colum\n",
    "#Data were split into chunks that don't split B + I sequences \n",
    "#and end with sentence-final punctuation marks (i.e., period, quesiton mark or exclamaiton point).\n",
    "final_lat_df['LAT Strings'] = final_lat_df['LAT Strings'].str.replace('B-','')\n",
    "final_lat_df['LAT Strings'] = final_lat_df['LAT Strings'].str.replace('I-','')\n",
    "\n",
    "#Check DF\n",
    "final_lat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download to csv\n",
    "final_lat_df.to_csv('citation_LATs_strings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download each row as a text named with the score and ID number \n",
    "texts = []\n",
    "for row in final_lat_df['LAT Strings'].items():\n",
    "    row_string = (str(row[1]))\n",
    "    texts.append(row_string)\n",
    "\n",
    "#Add filenames to list\n",
    "filenames = []\n",
    "for row in final_lat_df['Scores'].items():\n",
    "    row_string = (str(row[1]))\n",
    "    filenames.append(row_string)\n",
    "\n",
    "filenames[1]\n",
    "\n",
    "#Make new directory to store text files\n",
    "!mkdir lat_strings\n",
    "\n",
    "#Write texts to files\n",
    "n = 0\n",
    "for item in texts:\n",
    "  f = open(\"lat_strings/\" + filenames[n] +  '.txt','w')\n",
    "  n= n+1\n",
    "  f.write(item)\n",
    "  f.close()\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
