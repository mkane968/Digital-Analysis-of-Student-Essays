{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching Student Essays\n",
    "Using SpaCy for enrichment--lemmatization, part-of-speech tagging, and named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install os and glob\n",
    "import glob \n",
    "import os\n",
    "\n",
    "#Install pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Install numpy\n",
    "import numpy as np\n",
    "\n",
    "#Imports the Natural Language Toolkit, which is necessary to install NLTK packages and libraries\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "\n",
    "#Import matplotlib for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Imports spaCy itself, necessary to use features \n",
    "#!pip install spaCy\n",
    "import spacy\n",
    "#Load the natural language processing pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#Load spaCy visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import re  # For preprocessing\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import logging  # Setting up the loggings to monitor gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/Users/megankane/Desktop/clean_texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload dataframe of student essays (cleaned)\n",
    "df = pd.read_csv(r'cleaned_essays.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe for enrichment\n",
    "enriched_df = df[['ID', 'Clean_Text']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get lemmas\n",
    "lemma_list = []\n",
    "\n",
    "# Disable Dependency Parser, and NER since all we want is lemmatizer \n",
    "with nlp.disable_pipes('parser', 'ner'):\n",
    "  #Iterate through each doc object and getlemma, append lemma to list\n",
    "  for doc in nlp.pipe(enriched_df.Clean_Text.astype('unicode').values, batch_size=100):\n",
    "    word_list = []\n",
    "    for token in doc:\n",
    "        word_list.append(token.lemma_)\n",
    "        \n",
    "    lemma_list.append(word_list)\n",
    "\n",
    "#Put lemmas in a new column in dataframe\n",
    "enriched_df['Lemma_Text'] = lemma_list\n",
    "enriched_df['Lemma_Text'] = [' '.join(map(str, l)) for l in enriched_df['Lemma_Text']]\n",
    "\n",
    "#Check lemmas\n",
    "enriched_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get part of speech tags\n",
    "pos_list = []\n",
    "\n",
    "# Disable Dependency Parser, and NER since all we want is POS \n",
    "with nlp.disable_pipes('parser', 'ner'):\n",
    "  #Iterate through each doc object and tag POS, append POS to list\n",
    "  for doc in nlp.pipe(enriched_df.Clean_Text.astype('unicode').values, batch_size=100):\n",
    "    word_list = []\n",
    "    for token in doc:\n",
    "        word_list.append(token.pos_)\n",
    "        \n",
    "    pos_list.append(word_list)\n",
    "\n",
    "#Put POS in a new column in dataframe\n",
    "enriched_df['POS_Text'] = pos_list\n",
    "enriched_df['POS_Text'] = [' '.join(map(str, l)) for l in enriched_df['POS_Text']]\n",
    "\n",
    "#Check pos tags\n",
    "enriched_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get specific subset of part of speech tags\n",
    "propnoun_list = []\n",
    "\n",
    "# Disable Dependency Parser, and NER since all we want is POS \n",
    "with nlp.disable_pipes('parser', 'ner'):\n",
    "  #Iterate through each doc object and tag POS, append POS to list\n",
    "  for doc in nlp.pipe(enriched_df.Clean_Text.astype('unicode').values, batch_size=100):\n",
    "    word_list = []\n",
    "    for token in doc:\n",
    "      if token.pos_ == 'PROPN':\n",
    "        word_list.append(token)\n",
    "        \n",
    "    propnoun_list.append(word_list)\n",
    "\n",
    "#Make pos list a new column in DataFrame\n",
    "enriched_df['Proper_Nouns'] = propnoun_list\n",
    "enriched_df['Proper_Nouns'] = [', '.join(map(str, l)) for l in enriched_df['Proper_Nouns']]\n",
    "\n",
    "#Check proper noun tags\n",
    "enriched_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get named entities\n",
    "ent_list = []\n",
    "\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    for doc in nlp.pipe(enriched_df.Clean_Text.astype('unicode').values, batch_size=100):\n",
    "        ent_list.append(doc.ents)\n",
    "\n",
    "#Put NEs in a new column in dataframe\n",
    "enriched_df['NER_Text'] = ent_list\n",
    "enriched_df['NER_Text'] = [' '.join(map(str, l)) for l in enriched_df['NER_Text']]\n",
    "\n",
    "#Check named entities\n",
    "enriched_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Named Entity words\n",
    "ent_w_list = []\n",
    "\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    for doc in nlp.pipe(enriched_df.Clean_Text.astype('unicode').values, batch_size=100):\n",
    "        ent_w_list.append(doc.ents)\n",
    "\n",
    "enriched_df['NER_Words'] = ent_w_list\n",
    "enriched_df['NER_Words'] = [' '.join(map(str, l)) for l in enriched_df['NER_Words']]\n",
    "\n",
    "\n",
    "#Check named entities\n",
    "enriched_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download enriched texts to csv\n",
    "enriched_df.to_csv('enriched_texts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
