{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Segmenting Student Essays\n",
    "\n",
    "Perform basic cleaning measures (lowercasing, punctuation removal, whitespace removal, stopword removal) and segment full texts into paragraphs and sentences for further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install os and glob\n",
    "import glob \n",
    "import os\n",
    "\n",
    "#Install pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Install numpy\n",
    "import numpy as np\n",
    "\n",
    "#Imports the Natural Language Toolkit, which is necessary to install NLTK packages and libraries\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "\n",
    "#import regex\n",
    "import re\n",
    "\n",
    "#Installs libraries and packages to tokenize text\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from  nltk.text import ConcordanceIndex\n",
    "\n",
    "#Installs libraries and packages to clean text\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/Users/megankane/Desktop/Texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append all txt files to pandas dataframe\n",
    "\n",
    "#Make list for filenames and texts\n",
    "filenames = []\n",
    "data = []\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(f)]\n",
    "for f in files:\n",
    "    if f.endswith('.txt'):\n",
    "        with open (f, \"rb\") as myfile:\n",
    "            filenames.append(myfile.name)\n",
    "            data.append(myfile.read())\n",
    "d = {'ID':filenames,'Text':data}\n",
    "        \n",
    "essays = pd.DataFrame(d)\n",
    "essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Full Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
    "essays['Text'] = essays['Text'].apply(lambda x: x.decode('utf-8', errors='ignore'))\n",
    "essays['Text'] = essays['Text'].astype(str)\n",
    "\n",
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove newline characters and put in new column (will need to split paragraphs later)\n",
    "essays['Text_Newlines'] = essays['Text']\n",
    "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
    "essays['Text'] = essays['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase all words\n",
    "essays['Lower_Text'] = essays['Text'].str.lower()\n",
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "essays['NoPunct_Text'] = [p.sub(' ', x) for x in essays['Lower_Text'].tolist()]\n",
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extraneous whitespace using regular expressions\n",
    "essays['NoPunct_Text'] = essays['NoPunct_Text'] .str.replace('  +', ' ', regex=True)\n",
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers and extraneous characters\n",
    "essays['Clean_Text'] = essays['NoPunct_Text'] .str.replace('\\d+', '', regex=True)\n",
    "essays['Clean_Text'] = essays['Clean_Text'] .str.replace('_', '')\n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "essays['Text_NoStops'] = essays['Clean_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save cleaned dataframe to working directory\n",
    "essays.to_csv('cleaned_full_essays.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Segmentation and Cleaning\n",
    "Segment full texts into paragraphs by splitting on each newline character and performing basic cleaning procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only need one newlines version here\n",
    "paragraphs_df = essays[['ID', 'Text_Newlines']].copy()\n",
    "\n",
    "#Check new df\n",
    "paragraphs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of paragraphs in each text\n",
    "paragraph_counts = paragraphs_df['Text_Newlines'].str.count(r'\\n')\n",
    "paragraph_counts\n",
    "\n",
    "#Append paragraphs counts to dataframe\n",
    "paragraphs_df[\"Paragraph_Counts\"] = paragraph_counts\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new cell each time new paragraph starts \n",
    "new = paragraphs_df[\"Text_Newlines\"].str.split(r'\\n', expand = True).set_index(paragraphs_df['ID'])\n",
    "\n",
    "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
    "paragraphs_df = new.stack().reset_index()\n",
    "paragraphs_df.columns = [\"ID\", \"Paragraph\", \"Text\"]\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Filter out paragraphs with 5 or less words (headers)\n",
    "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.split().str.len().lt(10)]\n",
    "\n",
    "## Filter out paragraphs containing \"http://\", \"doi:\" , \"https://\" and \"://www\" (Works Cited citations)\n",
    "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"http://\")]\n",
    "\n",
    "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"https://\")]\n",
    "\n",
    "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"://www\")]\n",
    "\n",
    "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"www.\")]\n",
    "\n",
    "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\".com/\")]\n",
    "\n",
    "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"Vol.\")]\n",
    "\n",
    "paragraphs_df = paragraphs_df[~paragraphs_df['Text'].str.contains(\"doi:\")]\n",
    "\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform other basic cleaning procedures\n",
    "\n",
    "#Lowercase all words\n",
    "paragraphs_df['Lower_Text'] = paragraphs_df['Text'].str.lower()\n",
    "paragraphs_df.head()\n",
    "\n",
    "\n",
    "#Remove punctuation\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "paragraphs_df['NoPunct_Text'] = [p.sub(' ', x) for x in paragraphs_df['Lower_Text'].tolist()]\n",
    "paragraphs_df.head()\n",
    "\n",
    "#Remove extraneous whitespace using regular expressions\n",
    "paragraphs_df['NoPunct_Text'] = paragraphs_df['NoPunct_Text'] .str.replace('  +', ' ', regex=True)\n",
    "paragraphs_df.head()\n",
    "\n",
    "#Remove numbers and extraneous characters\n",
    "paragraphs_df['Clean_Text'] = paragraphs_df['NoPunct_Text'] .str.replace('\\d+', '', regex=True)\n",
    "paragraphs_df['Clean_Text'] = paragraphs_df['Clean_Text'] .str.replace('_', '')\n",
    "paragraphs_df\n",
    "\n",
    "#Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "paragraphs_df['Text_NoStops'] = paragraphs_df['Clean_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "paragraphs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make column with ID and paragraph\n",
    "paragraphs_df['ID_Paragraph'] = paragraphs_df['ID'].astype(str) + '_' + paragraphs_df['Paragraph'].astype(str)\n",
    "\n",
    "#Download paragraphs to csv\n",
    "paragraphs_df_download = paragraphs_df[['ID_Paragraph', 'Text']].copy()\n",
    "paragraphs_df_download.to_csv('all_paragraphs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download each paragraph as a txt file\n",
    "#Add each text to a new list called paragraphs\n",
    "paragraphs = []\n",
    "for row in paragraphs_df['Clean_Text'].items():\n",
    "    row_string = (str(row[1]))\n",
    "    paragraphs.append(row_string)\n",
    "\n",
    "#Add filenames to list\n",
    "filenames = []\n",
    "for row in paragraphs_df['ID_Paragraph'].items():\n",
    "    row_string = (str(row[1]))\n",
    "    filenames.append(row_string)\n",
    "\n",
    "filenames[1]\n",
    "\n",
    "#Make new directory to store text files\n",
    "!mkdir paragraphs_df\n",
    "\n",
    "#Write texts to files\n",
    "n = 0\n",
    "for item in paragraphs:\n",
    "  f = open(\"paragraphs_df/\" + filenames[n] +  '.txt','w')\n",
    "  n= n+1\n",
    "  f.write(item)\n",
    "  f.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation and Cleaning\n",
    "Segment paragraphs into sentences by splitting on each newline character and performing basic cleaning procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new df for sentence segmentation\n",
    "sentences_df = paragraphs_df.copy()\n",
    "\n",
    "#Count number of paragraphs in each text\n",
    "sentence_counts = sentences_df['Text'].str.count(r'[.!?]+')\n",
    "sentence_counts\n",
    "\n",
    "#Append paragraphs counts to dataframe\n",
    "sentences_df[\"Sentence_Counts\"] = sentence_counts\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new cell each time new sentence starts \n",
    "new = sentences_df.Text.str.split(r'[.!?]+', expand = True).set_index(paragraphs_df['ID'])\n",
    "\n",
    "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
    "sentences_df = new.stack().reset_index()\n",
    "sentences_df.columns = [\"ID\", \"Sentence\", \"Text\"]\n",
    "\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform basic cleaning procedures\n",
    "\n",
    "#Lowercase all words\n",
    "sentences_df['Lower_Text'] = sentences_df['Text'].str.lower()\n",
    "sentences_df.head()\n",
    "\n",
    "\n",
    "#Remove punctuation\n",
    "p = re.compile(r'[^\\w\\s]+')\n",
    "sentences_df['NoPunct_Text'] = [p.sub(' ', x) for x in sentences_df['Lower_Text'].tolist()]\n",
    "sentences_df.head()\n",
    "\n",
    "#Remove extraneous whitespace using regular expressions\n",
    "sentences_df['NoPunct_Text'] = sentences_df['NoPunct_Text'] .str.replace('  +', ' ', regex=True)\n",
    "sentences_df.head()\n",
    "\n",
    "#Remove numbers and extraneous characters\n",
    "sentences_df['Clean_Text'] = sentences_df['NoPunct_Text'] .str.replace('\\d+', '', regex=True)\n",
    "sentences_df['Clean_Text'] = sentences_df['Clean_Text'] .str.replace('_', '')\n",
    "sentences_df\n",
    "\n",
    "#Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentences_df['Text_NoStops'] = sentences_df['Clean_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make column with ID and sentence\n",
    "sentences_df['ID_Paragraph'] = sentences_df['ID'].astype(str) + '_' + sentences_df['Sentence'].astype(str)\n",
    "\n",
    "#Download paragraphs to csv\n",
    "sentences_df_download = paragraphs_df[['ID_Paragraph', 'Text']].copy()\n",
    "sentences_df_download.to_csv('all_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download each paragraph as a txt file\n",
    "#Add each text to a new list called paragraphs\n",
    "sentences = []\n",
    "for row in sentences_df['Clean_Text'].items():\n",
    "    row_string = (str(row[1]))\n",
    "    sentences.append(row_string)\n",
    "\n",
    "#Add filenames to list\n",
    "filenames = []\n",
    "for row in sentences_df['ID_Paragraph'].items():\n",
    "    row_string = (str(row[1]))\n",
    "    filenames.append(row_string)\n",
    "\n",
    "filenames[1]\n",
    "\n",
    "#Make new directory to store text files\n",
    "!mkdir sentences_df\n",
    "\n",
    "#Write texts to files\n",
    "n = 0\n",
    "for item in sentences:\n",
    "  f = open(\"sentences_df/\" + filenames[n] +  '.txt','w')\n",
    "  n= n+1\n",
    "  f.write(item)\n",
    "  f.close()\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
